\documentclass{article}
\usepackage[utf8x]{inputenc}
\title{AIND Research Review: Alpha Go}
\author{Stefan Heidekr√ºger}


%\usepackage
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amstext}
%\usepackage{graphicx}
%\usepackage{xfrac}
\usepackage[left=1in, right=1in, top=0.1in, bottom=1in]{geometry}
%\usepackage{hyperref}
%\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
%\usepackage{dsfont}
%\usepackage{courier}
%\usepackage{color}
\usepackage{amsthm}
%\usepackage[super]{nth}
\renewcommand{\qed}{\unskip\nobreak\quad\qedsymbol} %fixes position of \qed
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}}
\usepackage{rotating}
\usepackage{enumitem}
\usepackage{slashbox}
\usepackage[]{nomencl}
\usepackage{comment}
\usepackage[gen]{eurosym}
\usepackage{caption}
\usepackage{chngcntr}

\usepackage{xargs}                      % Use more than one optional parameter in a new commands
\usepackage[pdftex,dvipsnames]{xcolor}  % Coloured text etc.

\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}

\newcommandx{\notsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
%\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
%\newcommandx{\thiswillnotshow}[2][1=]{\todo[disable,#1]{#2}}

\usepackage{titlesec}
\setcounter{secnumdepth}{3}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\newcommand\inv[1]{#1^{-1}}
\newcommand\expect[2][]{\mathbb E_{#1}\left[#2 \right]}
\newcommand\card{\texttt{\#}}

%Define Theorem Environments
\theoremstyle{plain}
\newtheorem{thm}{Theorem}%[chapter]
\newtheorem{propos}[thm]{Proposition}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\theoremstyle{definition}
\newtheorem{defn}{Definition}[]
\newtheorem{ass}{Assumption}[]
\theoremstyle{remark}
\newtheorem*{exmpl}{Example}
\newtheorem*{remark}{Remark}
\newtheorem*{remarks}{Remarks}


\begin{document}
\maketitle

In \cite{silver2016mastering}, the authors describe the functionality, methodology and achievments of Google DeepMind's \emph{Alpha Go} Go-game playing agent (as of 2015). In essence, Alpha Go combines previously the methods of \emph{position evaluation (PE)} and \emph{Monte Carlo Tree Search (MCTS)} \textemdash both of which had previously been applied to Ga playing. Alpha Go's novelty lies in applying deep learning methods in order to find good heuristics for either of these methods, namely an approximate \emph{value function} $v \approx v^*$ in PE, and a succesfull \emph{policy} $p(a|s)$ for MCTS. The resulting agent vastly outperformed any previous computer Go agent and for the first time could match expert level human players.
In this review, we will very briefly outline the methods employed and the macro strategies of how the authors trained their heuristics.

\paragraph{Monte Carlo Tree Search}
MCTS is a sophisticated method of selectively building a search tree in game playing. While the traditional methods of minimax expansion simply build the \emph{entire} tree up to a certain depth that's dictated by time constraints (possibly with pruning, such as $\alpha$-$\beta$-pruning as applied in the lectures) and then evaluates these intermediate positions through a heuristic value function (see PE), MCTS selectively grows a few branches very deep - all the way to endgame, at the expense of less horizontal completeness. Simulated end-states are then sampled (hence Monte Carlo) in order to derive a winning probability for each legal move. In order to so, MCTS requires an appropriate sampling \emph{policy} to determine which branches to expand; given a current game state $s$ and the set of legal moves $\mathcal A$, a policy $p$ is a probability distribution on $\mathcal A$: 
$$p: \mathcal A \rightarrow [0,1], a \mapsto p(a|s)$$
A sucessfull policy should find the right balance between \emph{exploration}\textemdash exploring parts of the tree with little known information about them \textemdash and \emph{exploitation}\textemdash focussing on the most promising actions to maximize value. Alpha Go's policy network is found in two steps:
\begin{itemize}
	\item First, an "Expert Policy" $p_\sigma$ is trained through supervised learning on a database of human expert games. The corresponding NN architecture alternates between convolutional and relu layers (with a final softmax).
	\item Given $p_\sigma$ trained as above, the authors then train a second policy $p_\rho$ through reinforcement learning, by letting the current iteration of the game agent play agains previous iterations. The NN evaluating $p_\rho$ has the same architecture as above, with only the weights changed by the RL step. (The final version of Alpha Go replaces $p_\rho$ with a less accurate but much more efficient approximation $p_\pi$.)
\end{itemize}

\paragraph{Position Evaluation}
Due to the complexity of Go, it's prohibitive to expand branches until end-game, even when using MCTS. Therefore Alpha Go requires a position evaluation function $v$ in order to score the leaves of the tree expansion and propagate the scores back up the tree. The authors use reinforcement learning in order to learn an approximation $v_\theta$ to the true value function $v^*$ of the subgames. This is achieved through self-play of the agent using the previously trained policy $p_\rho$

The final, now world-famous agent Alpha Go finally plays by using $p_\pi$ for tree rollout and $v_\theta$ for leaf evaluation.

\nocite{*}
%\bibliographystyle{apalike}%amsalpha
\addcontentsline{toc}{chapter}{References} 
\bibliography{references}

\end{document}